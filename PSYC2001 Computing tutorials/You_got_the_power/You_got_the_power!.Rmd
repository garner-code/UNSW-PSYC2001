---
title: "you got the power!"
output: 
  html_document:
    theme: readable

---

This week, we are going to explore how improving our statistical power changes our ability to detect effects, if they exist.

## Checking installation and loading packages

As usual we first always check and load in our required packages.

```{r message = FALSE, warning= FALSE}
# Check if packages are installed, if not install.
if(!require(here)) install.packages('here') #checks if a package is installed and installs it if required.
if(!require(tidyverse)) install.packages('tidyverse')
if(!require(ggplot2)) install.packages('ggplot2')

library(here) #loads in the specified package
library(tidyverse)
library(ggplot2)
```

## Key power facts:

1. We can never know for sure what size our effect is going to be – unless we measured the entire population of interest – which is quite often THE ENTIRE HUMAN SPECIES. That would be a HUUUUUGE experiment. This means we have to make a theoretical or educated guess, and power our study accordingly.

2. The effect size we are hunting depends on the nature of the effect, not the size experiment we want to run. For example, the effect of getting the right nutrients on height is the same, regardless of how many people we sample in our experiment.

**Therefore**: our job as experimenters is to make the most educated guess that we can about the effect size we are looking for, and make sure our study is appropriately powered to give us the best chance to detect it, **if it even exists.**

The **best chance we have to change statistical power is to change our sample size.**

## Simulating sample sizes

In this first exercise we are going to simulate the impact of sample size on our experiment results using the `PSYC2001_global-time-on-social-data.csv`. We will learn about influence of sample size on **effect size and p value** using the `mean_time_on_social` variable.  Remember, that this variables stands for the following: 

-   `mean_time_on_social` – average hours/day on social media per experiment (self-report diary)

## Installing and loading new packages we will be using today

We will be using two new packages today. The `pwr` package to complete power calculations and the `effsize` package to generate effect sizes. Please run the code to install and load these packages.

```{r message = FALSE, warning= FALSE}
if(!require(effsize)) install.packages('effsize')
if(!require(pwr)) install.packages("pwr")

library(pwr)
library(effsize)
```


## Activity 1 - Simulating data 

Before we can simulate data we first need to load in our dataset. Can you help with this ? 

```{r}

#Use the read.csv() and here() functions to read in the dataset.

global_social_media <- read.csv(file = here("Data","PSYC2001_global-time-on-social-data.csv"))

```

To simulate our data we will be using our friend the `sample()` function from the previous tutorial. If you need a reminder on the usage of this function please click [here](#vectors-and-the-sample-function) or use the `?` syntax. 

Now that you are an expert on this function are you able to sample data for n = 10, n= 30, n= 50, n = 100, n= 200 and n = 500 ? Have a try at filling in the code block below:

```{r}
set.seed(1) #ensures we always get the same result from sampling ! 

sample_10 <- sample(global_social_media$mean_time_on_social, size = 10)
sample_30 <-sample(global_social_media$mean_time_on_social, size = 30)
sample_50 <-sample(global_social_media$mean_time_on_social, size = 50)
sample_100 <-sample(global_social_media$mean_time_on_social, size = 100)
sample_200 <-sample(global_social_media$mean_time_on_social, size = 200)
sample_500 <-sample(global_social_media$mean_time_on_social, size = 500)

```

Now that we have got our simulated data we are going to test whether the sample of `time-on-social` data is on average, above the international average of 2.2 hours. To do this we can run a one sample t-test against the test value of 2.2 for each of our simulated datasets. This will return a **p value** that we can use later on. We also want to get the **effect size** for each of these t-tests. To do this we will also using the `cohen.d()` function from the `effsize` package. It has the same arguments as the `t.test()` function as you will see below. 

## Activity 2 - Conducting t tests and effect sizes. 

Are you able to fill in the code below to help with this ? 

```{r}
t_test_10 <- t.test(sample_10 ~ 1, mu = 2.2)
eff_size_10 <- cohen.d(sample_10 ~ 1, mu = 2.2)

t_test_30 <- t.test(sample_30 ~ 1, mu = 2.2)
eff_size_30 <- cohen.d(sample_30 ~ 1, mu = 2.2)

t_test_50 <- t.test(sample_50 ~ 1, mu = 2.2)
eff_size_50 <- cohen.d(sample_50 ~ 1, mu = 2.2)

t_test_100 <- t.test(sample_100 ~ 1, mu = 2.2)
eff_size_100 <- cohen.d(sample_100 ~ 1, mu = 2.2)

t_test_200 <- t.test(sample_200 ~ 1, mu = 2.2)
eff_size_200 <- cohen.d(sample_200 ~ 1, mu = 2.2)

t_test_500 <- t.test(sample_500 ~ 1, mu = 2.2)
eff_size_500 <- cohen.d(sample_500 ~ 1, mu = 2.2)
```

At this point it is worth knowing that running the `t.test()` function returns a lot of different things. Run the code block below to have a look. 

```{r}
view(t_test_10)
```

The same thing applies to running the `cohen.d()` function. Lets have a look at what it returns.

```{r}
view(eff_size_10)
```

In this case we only want the `p-value` from the `t.test()` output and the `estimate` (cohens.d) from the `cohen.d()` output. We can extract these by using the `$` syntax to pull the `p-value` and `estimate`.

Please try to help with this below:

```{r}
p_value_10 <- t_test_10$p.value
p_value_30 <- t_test_30$p.value
p_value_50 <- t_test_50$p.value
p_value_100 <- t_test_100$p.value
p_value_200 <- t_test_200$p.value
p_value_500 <- t_test_500$p.value

cohen_d_10 <- eff_size_10$estimate
cohen_d_30 <- eff_size_30$estimate
cohen_d_50 <- eff_size_50$estimate
cohen_d_100 <- eff_size_100$estimate
cohen_d_200 <- eff_size_200$estimate
cohen_d_500 <- eff_size_500$estimate
```

Well done ! This was really difficult and you have done a great job simulating this data. We now have a **p-value** and **effect size** for our different sample sizes. 

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Crappy code be like"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/You_got_the_power/Terrible%20code.gif?raw=true")
```


::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong>There is a lot of repeated code here. Good coding practice generally suggests to reduce the amount of repeated code to improve the readability and efficiency of your code (not to mention how long it takes to write !). One way to do this is by using the `sapply()` function which loops or repeats functions (or functions of functions !). For more on that please go [here](https://nicercode.github.io/guides/repeating-things/).
:::

## Activity 3 - Plotting our simulated data

Now let's see what we can learn from our simulation results. First lets combine our **p-values** and **effect sizes** into a dataframe so it can be plotted. The simplest approach is to start by organising the results into three vectors using the `c()` syntax:

1. **p-values vector** – stores all of the p-values in order.

2. **effect sizes vector** – stores the corresponding effect sizes in the same order.

3. **sample sizes vector** – stores the number of samples for each test, also in the same order.

Each position across these three vectors matches up. For example, the first entry in all three vectors comes from the same test, the second entry from the next test, and so on. The sample size vector acts like a label, letting you track which **p-value** and **effect size** came from which sample size. 

```{r}
sample_size <- c(10,20,30,100,200,500) #the c() syntax concatenates values into a vector
p_values <- c(p_value_10,p_value_30,p_value_50,p_value_100,p_value_200,p_value_500)
cohens_ds <- c(cohen_d_10,cohen_d_30,cohen_d_50,cohen_d_100,cohen_d_200,cohen_d_500)
```

We can now combine these into a single dataframe using the `data.frame()` function. Note we explicitly use the `factor()` function here to make sure that R knows that our sample size is a factor not a numeric variable. 

```{r}
samples_df <-data.frame(sample_size = factor(sample_size), #here we make sure to explicitly tell R that sample_size is a factor and not a numeric (which would result in weird things!)
              p_values = p_values,
              cohens_ds = cohens_ds)


```

Finally, are you able to use your growing `ggplot` prowess to plot how the **p-value** and **effect size** changes across sample sizes ? 

```{r}
samples_df %>% 
  ggplot(aes(x = sample_size, y = p_values)) +
  geom_col(fill = "blue") +
  labs(x = "Sample Size", y = "p value") +
  theme_classic()


```

```{r}
samples_df %>% 
  ggplot(aes(x = sample_size, y = cohens_ds)) +
  geom_col(fill = "red") +
  labs(x = "Sample Size", y = "p value") +
  theme_classic()
```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> How do the effect sizes change across N, and why might this be? How do the p-values change across N, and why might this be? What have you learned about using statistical power in experiment design, after 1 & 2?
:::

## Power analysis in R

Now that we have figured out that the **main purpose of a power analysis is to calculate the sample size required to detect an effect of interest** let's see how we can do this easily in R.

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="The power of power analyses"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/You_got_the_power/The%20power%20of%20power%20analyses.jpg?raw=true")
```


## Power analysis on correlations

First let's figure out how to do power analysis on correlations. When conducting a power analysis for correlations (as should be clear from your lectures and tutorials on power analysis) there are a few important ingredients. 

1.  **Desired power**. Lets say we want 90% power to detect our effect.
2.  **Alpha level**. This is conventionally set to 0.05. 
3.  **Correlation coefficient 'r'**. Lets say that we are looking for a correlation of r = 0.5
  
Now that we have these parameters we can perform a power calculation using the `pwr.r.test` function. 
  
```{r}
pwr.r.test(r = 0.50, sig.level = 0.05, power = 0.9)
```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What is the takeaway from this output ? What sample size is required to detect our effect of interest?
:::

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> We can also use this package to do power calculations for other tests like t-tests and regressions.
:::


## Activity 4 - Conducting a power analysis for a t-test

We can also use the power analysis functions from the `pwr` package to perform power analyses for other types of statistical tests. For instance, we can conduct a power analysis for an independent samples t-test using the function `pwr.t.test`. 

When performing a power analysis on a t-test the ingredients required are mostly the same.

1.  **Desired power**. Lets say we want 90% power to detect our effect.
2.  **Alpha level**. This is conventionally set to 0.05. 
3.  **Effect size** . Lets say that we are looking for an effect size of d = 0.5.

Please fill in the code below to determine the sample size. 

```{r}
pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.9, type = "two.sample")
```

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> You will notice that instead of using gamma ($\gamma$) like we do for handwritten calculations we use cohens d in the `pwr` package. Both measures are valid measures of the size of effect and can be used in power calculations. 
:::


## Writing up a power analysis

Let's have a go at writing out the results and conclusions from this power analysis

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Results:</strong> A power analysis was performed to determine the minimal sample size required to detect a medium effect size with 90% power. Results indicated that the minimum sample size required to achieve 90% power for detecting a medium effect size (d = 0.50) at a significance level of alpha = .05 (two tailed) was 86 participantsper group for an independent samples t-test.
:::

Now have a go at writing the conclusion yourself. You got this ! 

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Conclusions:</strong> Your conclusion will go here !
:::

## You are Free!

Well done finishing this tutorial. You've got so much power now. We will see you all next week for more computing fun !

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="The power of power analyses"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/You_got_the_power/Power.jpg?raw=true")
```

