--- 
title: "UNSW PSYC2001 Computing Tutorials"
author: "Bart Cooley, Peter Lovibond, Kelly Garner"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This book contains the complete guide for computing tutorials for PSYC2001 at UNSW.
link-citations: yes
github-repo: rstudio/bookdown-demo
---

# Course Information {-}

## The purpose of this coursebook

Welcome to the computing tutorials for PSYC2001. This coursebook has been designed to support your learning of statistics through hands-on practice with R. It accompanies the RMarkdown tutorial documents available on Moodle and will guide you through the more code-heavy components of the course. The focus is not just on memorising but on learning how to think about data, ask good questions, and use R as a tool to answer them. 

## Structure of the Coursebook

Each chapter introduces a key concept in statistics. You will often see worked examples with code that runs in R. These examples show you how to apply statistical techniques step by step. Throughout the chapters, you will also find incomplete code blocks. These are practice opportunities for you to fill in the missing parts. Completing these code blocks is important, because your goal at the end of each tutorial is to be able to reproduce the coursebook chapter using the RMarkdown code. More importantly, it will help you move from simply reading code to actively writing and using it. Throughout this course book there will be questions that encourage you to think about about or apply what you've learned in new situation. 

<!--chapter:end:index.Rmd-->

# Introduction to R

To perform data analysis in Psychology, one needs some powerful software to help you get data into shape, and to apply all the fancy statistical tests that you will learn about in this course. In this course, we will be using the programming language R and the software R Studio to do this.

## R and R Studio

For this course, you need two different bits of software, [R](https://www.r-project.org/) and [RStudio](https://www.rstudio.com/products/rstudio/download/#download). R is a programming language that you will write code in and R Studio is an Integrated Development Environment (IDE) which makes working with R easier. Think of it as knowing English and using a plain text editor like NotePad to write a book versus using a word processor like Microsoft Word. You could do it, but it wouldn't look as good and it would be much harder without things like spell-checking and formatting. In a similar way, you can use R without R Studio but we wouldn't recommend it. The key thing to remember is that although you will do all of your work using R Studio for this course, you are actually using two pieces of software which means that from time-to-time, both of them may have separate updates.

R is a free and open-source programming language that is widely used for statistical computing and data analysis. R Studio is a user-friendly interface that makes it easier to write and run R code, manage files, and visualize data.

All of the School of Psychology computers have R and R Studio installed, however, we can only guarantee that the computers in the Level 2 psychology labs have the right set-up. 

Both R and RStudio are freely available so you may wish to install them on your own machine. There is a useful [guide to installing them both here](https://psyteachr.github.io/hack-your-data/r_instructions.html) that you can use. Note that the PSYC2001 staff are unable to help you if you have specific technical issues setting up R and RStudio on your own machine. But the tutors can support you using R and RStudio on the School of Psychology computers in the computer labs. If you are having specific technical issues setting this up on your own machine, then you should submit an issue to UNSW IT Services.



## Getting to know R Studio

R Studio has a console that you can try out code in (appearing as the bottom left window in **Figure 1**, there is a script editor (top left), a window showing functions and objects you have created in the “Environment” tab (top right window in the figure), and a window that shows plots, files packages, and help documentation (bottom right).


```{r img-rstudio, echo=FALSE, fig.align='center', fig.cap="RStudio interface"}

knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Introduction_to_R/RStudio.png?raw=true")

```


## Functions and arguments

**Functions** in R allow you to perform tasks that would take a long time to write out by hand, by only using one term! Think of them as nifty tools that will save you lots of typing, again and again. A function normally takes a number of **arguments** (you might want to think as these as verbs that require a subject and an object). You can look up all the arguments that a function takes by using the help documentation by using the format `?function`. Some arguments are required, and some are optional. Optional arguments will often use a default (normally specified in the help documentation) if you do not enter any value.

As an example, let’s look at the help documentation for the function `rnorm()` which randomly generates a set of numbers with a normal distribution. 



## Activity 1

Open up R Studio and in the console, type the following code:  

```{r help-doc, eval=FALSE}

?rnorm

```

The help documentation for `rnorm()` should appear in the bottom right help panel. We know that the documentation for rnorm can be a little bit... confusing. To simplify the `rnorm()` function is used to simulate data points from a normal distribution with a given mean and standard deviation. Do not worry about pnorm/dnorm, they can remain as mythical functions in the interest of simplicity.

In the usage section, we see that `rnorm()` takes the following form:

```{r arguments, eval = FALSE}
rnorm(n, mean = 0, sd = 1)
```

In the arguments section, there are explanations for each of the arguments. `n` is the number of observations we want to create, `mean` is the mean of the data points we will create and `sd` is the standard deviation of the set. In the details section it notes that if no values are entered for `mean` and `sd` it will use a default of 0 and 1 for these values. Because there is no default value for `n` it must be specified otherwise the code won't run.

Let's try an example and just change the required argument `n` to ask R to produce 5 random numbers. 



## Activity 2

* Copy and paste the following code into the console.  

```{r rnorm-n}
set.seed(12042016)
rnorm(n = 5)
```

These numbers have a mean of 0 and an SD of 1. Now we can change the additional arguments to produce a different set of numbers.

```{r}
rnorm(n = 5, mean = 10, sd = 2)
```

This time R has still produced 5 random numbers, but now this set of numbers has a mean of 10 and an sd of 2 as specified. Always remember to use the help documentation to help you understand what arguments a function requires.



## Argument names

In the above examples, we have written out the argument names in our code (e.g., `n`, `mean`, `sd`), however, this is not strictly necessary. The following two lines of code would both produce the same result (although each time you run `rnorm()` it will produce a slightly different set of numbers, because it's random, but they would still have the same mean and SD):

```{r argument-names, eval = FALSE}

rnorm(n = 6, mean = 3, sd = 1)
rnorm(6, 3, 1)

```

Importantly, if you do not write out the argument names, R will use the default order of arguments, that is for `rnorm` it will assume that the first number you enter is `n`. the second number is `mean` and the third number is `sd`. 

If you write out the argument names then you can write the arguments in whatever order you like:

```{r argument-order, eval = FALSE}

rnorm(sd = 1, n = 6, mean = 3)

```

When you are first learning R, you may find it useful to write out the argument names as it can help you remember and understand what each part of the function is doing. However, as your skills progress you may find it quicker to omit the argument names and you will also see examples of code online that do not use argument names so it is important to be able to understand which argument each bit of code is referring to (or look up the help documentation to check).

In this course, we will always write out the argument names the first time we use each function, however, in subsequent uses they may be omitted.



## Tab auto-complete

One very useful feature of R Studio is the tab auto-complete for functions (**see Figure 2: Tab auto-complete**). If you write the name of the function and then press the tab key, R Studio will show you the arguments that function takes along with a brief description. If you press enter on the argument name it will fill in the name for you, just like auto-complete on your phone. This is incredibly useful when you are first learning R and you should remember to use this feature frequently. 

```{r img-autocomplete, echo=FALSE, fig.align='center', fig.cap="Tab auto-complete"}

knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Introduction_to_R/autocomplete.png?raw=true")

```



## Base R and packages

When you install R you will have access to a range of functions including options for data wrangling and statistical analysis. The functions that are included in the default installation are typically referred to as **Base R** and there is a useful cheat sheet that shows many Base R functions [here](https://stat.ethz.ch/R-manual/R-devel/library/base/html/00Index.html).

However, the power of R is that it is extendable and open source - put simply, if a function doesn't exist or doesn't work very well, anyone can create a new **package** that contains data and code to allow you to perform new tasks. You may find it useful to think of Base R as the default apps that come on your phone and packages as additional apps that you need to download separately.

## Installing and loading packages

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> The UNSW psychology computers will already have all of the packages you need for this course so you only need to install packages if you are using your own machine.
:::

## Activity 3: Install the tidyverse

In order to use a package, you must first install it. The following code installs the package `tidyverse`, a package we will use very frequently in this course.

If you are interested in learning more about tidyverse and how incredibly useful it is in R, please consider reading [R for Data Science](https://r4ds.hadley.nz/)

* If you are working on your own computer, use the below code to install the tidyverse. **Do not do this if you are working on a University machine**.  

```{r install-packages, eval = FALSE}
install.packages("tidyverse")
```

You only need to install a package once, however, each time you start R you need to load the packages you want to use, in a similar way that you need to install an app on your phone once, but you need to open it every time you want to use it.

To load packages we use the function `library()`. Typically you would start any analysis script by loading all of the packages you need, but we will come back to that in the labs.



## Activity 4: Load the tidyverse

* Run the below code to load the tidyverse. You can do this regardless of whether you are using your own computer or a University machine.  

```{r library-load, eval = FALSE}
library(tidyverse)
```

You will get what looks like an error message - it's not. It's just R telling you what it's done.

Now that we've loaded the `tidyverse` package we can use any of the functions it contains but remember, you need to run the `library()` function every time you start R.



## Package updates

In addition to updates to R and R Studio, the creators of packages also sometimes update their code. This can be to add functions to a package, or it can be to fix errors. One thing to avoid is unintentionally updating an installed package. When you run `install.packages()` it will always install the latest version of the package and it will overwrite any older versions you may have installed. Sometimes this isn't a problem, however, sometimes you will find that the update means your code no longer works as the package has changed substantially. It is possible to revert back to an older version of a package but try to avoid this anyway.

::: {style="border-left: 4px solid #FF9800; background-color: #FFF3E0; padding: 10px; margin: 10px 0;"}
<strong>Warning:</strong> To avoid accidentally overwriting a package with a later version, you should **never** include `install.packages()` in your analysis scripts in case you, or someone else runs the code by mistake. Remember, the UNSW psychology computers will already have all of the packages you need for this course so you only need to install packages if you are using your own machine.
:::

## Package conflicts

There are thousands of different R packages with even more functions. Unfortunately, sometimes different packages have the same function names. For example, the packages `dplyr` and `MASS` both have a function named `select()`. If you load both of these packages, R will produce a warning telling you that there is a conflict.

```{r package-conflict}
library(dplyr)
library(MASS)
```

In this case, R is telling you that the function `select()` in the `dplyr` package is being hidden (or 'masked') by another function with the same name. If you were to try and use `select()`, R would use the function from the package that was loaded most recently - in this case it would use the function from `MASS`.

If you want to specify which package you want to use for a particular function you can use code in the format `package::function`, for example:

```{r package-specify, eval=FALSE}
dplyr::select()
MASS::select()
```


```{r echo = FALSE}
detach("package:MASS", unload = TRUE)
```


Note that this is for your own information, so that you aren't alarmed when you see such messages in your great future of data analysis. You won't have to worry about package conflicts during this course.


## Objects

A large part of your coding for data analysis will involve creating and manipulating objects. Objects contain stuff. That stuff can be numbers, words, or the result of operations and analyses.You assign content to an object using `<-`.

## Activity 5: Create some objects

* Copy and paste the following code into the console, change the code so that it uses your own name and age and run it. You should see that `name`, `age`, `today`, `new_year`, and `data` appear in the environment pane.  

```{r objects}

name <- "emily"
age <- 15 + 18 
today <-Sys.Date()
new_year <- as.Date("2020-01-01")
data <- rnorm(n = 10, mean = 15, sd = 3)

```

```{r img-objects-enviro, echo=FALSE,fig.align='center',fig.cap="Objects in the environment"}

knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Introduction_to_R/objects-enviro.png?raw=true")

```

Note that in these examples, `name`,`age`, and `new_year` would always contain the values `emily`, `33`, and the date of New Year's Day 2020, however, `today` will draw the date from the operating system and `data` will be a randomly generated set of data so the values of these objects will not be static.

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Warning:</strong> You may also see objects referred to as 'variables'. There is a difference between the two in programming terms, however, they are used synonymously very frequently.
:::

As a side note, if you ever have to teach programming and statistics, don't use your age as an example because everytime you have to update your teaching materials you get a reminder of the fragility of existence and your advancing age. 

Importantly, objects can be involved in calculations and can interact with each other. For example:

```{r objects-interact}

age + 10
new_year - today
mean(data)

```

Finally, you can store the result of these operations in a new object:

```{r objects-interact2}

decade <- age + 10

```


::: {style="border-left: 4px solid #00bfa5; background-color: #e0f2f1; padding: 10px; margin: 10px 0;"}
<strong>Tip:</strong> You may find it helpful to read `<-` as `contains`, e.g., `name` contains the text `emily`.
:::

## Datatypes

You will constantly be creating objects throughout this course and you will learn more about them and how they behave as we go along, however, for now it is enough to understand that they are a way of saving values, that these values can be numbers, text, or the result of operations, and that they can be used in further operations to create new variables. For now, we can have a look at the datatypes of our objects using the function `typeof`.

```{r}
#These are both doubles (i.e numbers!)
typeof(age)
typeof(new_year)

#This is a chr (i.e contains letters!)
typeof(name)


```

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> There are 5 main datatypes: double, integer, complex, logical and character. For historic reasons, double is also called numeric. We will learn about and use many of these different datatypes in this course.
:::

We will learn about using these datatypes (as well as some of the others) throughout this course, so don't fret if you don't understand it yet !

## Looking after the environment

If you've been writing a lot of code you may find that the environment pane (or workspace) has become cluttered with many objects. This can make it difficult to figure out which object you need and therefore you run the risk of using the wrong data frame. If you're working on a new dataset, or if you've tried lots of different code before getting the final version, it is good practice to remember to clear the environment to avoid using the wrong object. You can do this in several way.

1. To remove individual objects, you can type `rm(object_name)` in the console. Try this now to remove one of the objects you created in the previous section. 
2. To clear all objects from the environment run `rm(list = ls())` in the console.
3. To clear all objects from the environment you can also click the broom icon in the environment pane. 


```{r echo=FALSE, fig.align='center', fig.cap="Clearing the workspace"}

knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Introduction_to_R/broom.png?raw=true")

```

## You are Free!

Congratulations you have survived the first tutorial for this course. We look forward to seeing you next week for more coding fun. 



```{r  echo=FALSE, fig.align='center', fig.cap="Students reaction to finishing this"}

knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Introduction_to_R/broom.png?raw=true")

```



<!--chapter:end:Introduction_to_R.Rmd-->

# Data wrangling and visualisation

## Checking installation and loading packages

Before we can **begin** any script we first need to make sure that the **required packages are installed** in our version of RStudio. Next, we can **load the required packages** to be used in the script. The code block below will do this for you.

Note that in the code below, we've added a fancy `if` statement to check if the packages are installed. If they are not installed, it will install them for you. This is a good practice to ensure that your code runs smoothly on any computer without needing to manually install packages. Particularly if you are using the University's computers, where we have already installed the packages you need (and we ask you not to install any more!).

```{r  message = FALSE, warning= FALSE}
# Check if packages are installed, if not install.
if(!require(here)) install.packages('here') #checks if a package is installed and installs it if required.
if(!require(tidyverse)) install.packages('tidyverse')
if(!require(ggplot2)) install.packages('ggplot2')

library(here) #loads in the specified package
library(tidyverse)
library(ggplot2)
```

## What do packages do?

You should be able to see that we have installed and loaded **3 different packages**. Let's first go over the basics of what a package is. In its simplest terms, a **package is a toolbox** that someone has created for us in R that **makes our life easier**. These packages build on the basic code that comes with the R programming language (what **RStudio** uses to run), called `base R`.

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Opening an R package"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Data_wrangling_and_visualization/Opening%20an%20R%20package.gif?raw=true")
```

## What do these packages do?

It is always a **good idea to check the documentation** for a package before you use it. We can do this by using the **help syntax**, which is the `?`. The package we are trying to get help with is called `here`. Try to run this code by **clicking on the green arrow on the corner of the code block on the left side of your screen**. This will open a webpage that tells us the **purpose** of the `here` package and how it works.

```{r, echo=FALSE, out.width="100%", fig.align='center', fig.cap="Running code in R"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Data_wrangling_and_visualization/Example%20of%20running%20code.jpg?raw=true")
```


```{r, message=FALSE, eval=FALSE}
?here #? loads the documentation for a specified package.
```

## Activity 1 - Using help syntax

**Fill in the code block below** by putting in the **help syntax** `?` and the name of the package you are interested in. This will get the documentation for the **other packages** we are using. You can do this by substituting in the packages that we are using from above. Have a read of each of these pages and click on any links you find interesting. These are the **main packages** we will be using throughout this course.

```{r, eval=FALSE, echo = TRUE}
# Try to use the help function '?' to read more about the packages we are using today
# The packages we are using are 'tidyverse' and 'ggplot2'.

?tidyverse
?ggplot2
```

## Organisation and CSV files

The dataset we are using should be downloaded from the tutorial folder on Moodle. On your computer navigate to this folder and have a look at what it contains.

You should note that it contains the following:

- An R Project file called **'Data_wrangling_and_visualization.Rproj'**
- An R Markdown file called  **'Data_wrangling_and_visualization.Rmd'**.
- A text file called **README.txt'**
- A folder called **data**, containing a .csv file called **'PSYC2001_social-media-data.csv'**
- A folder called **output**, which is where all your output will go. 
- A folder called **scripts** which is where all new scripts will go.  

These are the **key ingredients needed to organise all projects in R**.


```{r, echo=FALSE, out.width="100%", fig.align='center', fig.cap="Project Organisation"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Data_wrangling_and_visualization/Example%20of%20folder.JPG?raw=true")
```


You will notice that the data for today, called `PSYC2001_social-media-data.csv`, is a **csv file** (short for a *Comma Separated Value* file). 

csv files are a common and handy way to store data, because they can be read by heaps of different programs, like Excel, Google Sheets, and R. They are simple text files where each line represents a row of data, and the values in each row are separated by commas. Try double clicking on the csv file now to see what happens. You'll see that Excel offers to open it for you. Click ok to open the file in Excel and take a look at how the data looks. Then say 'that's very kind of you Excel, but we are more powerful than you. We R.'

```{r, echo=FALSE, out.width="100%", fig.align='center', fig.cap="Social Media Data in Excel"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Data_wrangling_and_visualization/Data%20in%20excel.JPG?raw=true")
```

## Importing your Data in R 

We are now going to load our first dataset into R. To do this we will need to **import the dataset** using a function capable of importing csv files. 

We will be using **two different functions** to achieve this. The `read.csv()` function is used to import our csv dataset and it comes from the `utils` package which is part of `base R`. But the `read.csv()` function needs to know where the file lives on the computer. To do this, we use the `here()` function from the `here` package. This function tells R the **location of the project** we are working from, to make locating the data easier.

Let's first confirm that `here()` knows our current location on this PC (called the **'Working Directory'**)

```{r}
here()
```

We can use this to easily find where our file is located and read it. 

```{r, results='hide', message=FALSE, warning=FALSE}
social_media <- read.csv(file = here("Data","PSYC2001_social-media-data.csv")) #reads in csv files

```

Note that this code works because 'Data' is a folder in the same directory as the `.Rproj` file. If the folder Data was somewhere else, then using `here()` would not work. We would need to instead specify the full path to the file. If you are unsure what we mean by 'file path' then please google it, google it now.

::: {style="border-left: 4px solid #FF9800; background-color: #FFF3E0; padding: 10px; margin: 10px 0;"}
<strong>Warning:</strong> If you have an error, something has gone wrong—please ask your tutor for help!
:::

## Having a look at our imported data

Our data should now be **imported into R!**

Recall from the `README.txt` file (**make sure you read this**) that this dataset was collected as part of an experiment investigating social media use in young Australian adults. Sixty young adults answered questions about their **social media usage** as well as their **political attitudes**. Data about their **social media usage** (e.g., likes) was collected while they used their preferred platforms under various conditions.

The variables in the data are:

- `id` – a unique identifier (S1–S60)
- `age` – age in years
- `time_on_social` – average hours/day on social media (self-report diary)
- `urban` – urban (1) or rural (2) area (based on postcode density)
-` good_mood_likes` – likes/10 min during a good mood (from platform + diary)
- `bad_mood_likes` – as above, but during bad mood
- `followers` – average number of followers across platforms

The next 3 columns are political attitude subscales:

- `informed` – how politically informed they feel (e.g., read news daily)
- `campaign` – how much they engage in campaign-related discussion
- `activism` – involvement in activism (e.g., protests, petitions)

We should now check that we have **imported into Rstudio matches this description (and what we saw when we opened it in excel)**.  There are a couple of ways to do this.

- The first way is to **manually click** through to the dataset. You can do this by:
    1. Clicking on **Environment** in the top right section of your screen.
    2. Clicking on **social_media**.
    3. You should see a new tab pop up with the data in a **table-like format** (this is called a **dataframe**).
    4. Make sure that this new tab looks similar to what you saw when you opened Excel file.

```{r, echo=FALSE, out.width="200%", fig.align='center', fig.cap="Navigating to dataset"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Data_wrangling_and_visualization/Directing%20students%20to%20view%20data%20in%20R.gif?raw=true")
```

- We can also do this **programmatically** using the code blocks below:

```{r, eval=FALSE}
# Method 1 - Type in the name of the object
social_media
```

```{r}
# Method 2 - Use the View function
View(social_media) #view automatically displays the dataset in a tab.

```

```{r}
# Method 3 - Use the head function
head(social_media) #head displays the first 6 rows of each variable.

```

```{r}
# Method 4 - Use the str function
str(social_media) #displays an overall summary of the object and variable structure.

```

You should now have a good idea of what `PSYC2001_social-media.csv` looks like in RStudio. This should match what we saw in excel.


You will also notice that the last function, `str()`, displays a **summary** of the object. This includes:

- The **object type** (a **dataframe**)
- The **number of observations/rows** (60)
- The **number of variables/columns** (10)
- The **datatype**: **chr** for **`id`**, and **num** for all other variables

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Please discuss with your deskmate and tutor what you think **chr** and **num** mean.
:::

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="You thinking"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Data_wrangling_and_visualization/Students%20thinking.gif?raw=true")
```

## Checking the quality of our data

Once we have imported our dataset into R, it’s important to check the quality of the data. One simple way to do this is by using the `summary()` function. 

```{r}
summary(social_media) #summary provides a quick overview of the data in each variable. 
```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Do you notice anything unusual in the output of this data ? Discuss with your neighbour and tutor.
:::

::: {style="border-left: 4px solid #00bfa5; background-color: #e0f2f1; padding: 10px; margin: 10px 0;"}
<strong>Hint:</strong> Take a closer look at the `time_on_social` variable.
:::

## Cleaning the data

It should now be clear that this data is unusual because it has a **minimum value** of `-999` in the `time_on_social` variable which is measured in hours (we can't have negative time !).

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Back to the future !"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Data_wrangling_and_visualization/Back%20To%20The%20Future%20GIF.gif?raw=true")
```

A good question to ask now is - **why are these values in the dataset?**

Sometimes when collecting data, we can’t get a response from every participant. Instead of leaving a blank, researchers will sometimes put in a placeholder value like `-999` to show that the data is missing. These aren't real numbers; they just mean the data wasn’t recorded. But `-999` isn’t the standard way to show missing data in R. R uses `NA` to represent missing values, and that’s important because most R functions know how to handle `NA` properly but they don’t know to ignore `-999`.

Lets first have a look at how many `-999` values are present in the data. We can do this by using the `filter()` function from the `tidyverse` package which is used to keep (or remove) rows based on certain conditions. 

```{r filtering values without piping}

social_media_filtered <- filter(social_media, time_on_social == -999) #keep all rows where `time_on_social` is equal to -999
View(social_media_filtered) #view the filtered dataframe
```

Handily, we can then use the `count()` function from the `tidyverse` package to sum the number of rows in the resulting dataframe.

```{r counting rows}
count(social_media_filtered) #count the number of rows in the filtered dataframe)
```

## Introducing Piping

A short aside to introduce a **very specical operation called a 'pipe'** or `%>%`. This operation is part of the `tidyverse` package and allows you to pass the result from one function to the next seamlessly in a sort of assembly-line like fashion. **Throughout the rest of the course we will be using 'piping' as it is easier to follow and code**. For instance, lets repeat what we just did above but with pipes instead. 

```{r filtering values with pipes}
social_media %>% #pass the values from social_media to the filter function
  filter(time_on_social == -999) %>% #keep all rows that are equal to -999 and pass the result to count
  count() #count the number of remaining columns

```

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> Piping is not friends with every function. Some functions will not accept inputs from pipes (no matter how nice they are !). This will become clearer as we code throughout this course.
:::

Now lets use a piping method to clean this data up by replacing `-999` values  with more R readable `NA` values. `NA` stands for 'Not Available' and is the standard way to represent missing data in R. We can do this using the `mutate()` and `na_if()` functions from the `tidyverse` package. The `mutate()` function is used to alter or make new columns in a dataframe based on the conditions we specify and `na_if()` is used to replace given values with `NA` in a dataframe.

```{r}
social_media_NA <- social_media %>%
  mutate(time_on_social = na_if(time_on_social,-999)) #mutate alters columns and rows. na_if replaces -999 with NA
                                                    
```

## Exporting Data

It would be a good idea to save this dataset for future tutorials, so that we don't have to replace `-999` values with `NA` values every single time.

We can do this with the `write.csv()` function from `baseR`. This function takes a dataframe in R and saves it as a .csv file on your computer. Later, we can simply read that csv back into R, and it will already be cleaned.

```{r Saving datasets}

write.csv(social_media_NA, here("Output","PSYC2001_social-media-data-cleaned.csv")) #creates a csv file from the dataframe social_media_NA


```

## Data visualization using `ggplot2`

Visualizing data is a crucial step in data analysis. You should never run a statistical analysis without first visualising your data. It helps us understand the distribution of our data, identify patterns, and communicate our findings effectively. It also helps us identify whether the data is suitable for the analysis we want to perform, or whether some weird values remain that could influence the result of our statistical tests, and even worse, our interpretations!

So, let's look at some data! We’re going to start by visualising the `time_on_social` variable.

To do this we will need to use the `ggplot()` function. This is the main function from the `ggplot2` package (**you should know what this is from reading the documentation**). `ggplot()` provides the canvas of the graph you want to make. 
To make the basic canvas `ggplot()` requires two things:

1. The data that you want it to plot.

2. The variables to go on the x and y axes.

Importantly, `ggplot()` only provides the canvas. It does not draw anything by itself. You have to add layers to the canvas created by `ggplot()` by using other functions that can create bars, points or lines ! 

Here we use `geom_boxplot()` which creates a boxplot for us. What is a boxplot you ask ? A boxplot is a graph that shows the spread of data points where the lower part of the "box" represents the bottom quartile (where 25% of the data lies), the upper part of the box represents the upper quartile (where 75% of the data lies) and the middle of the box represents the median (the middle value). The "whiskers" (vertical lines) extend to the smallest and largest values not considered outliters.

```{r}
social_media_NA %>%
ggplot(aes(y = time_on_social),) + #ggplot uses aesthetic (aes()) to map axes. 
  scale_x_discrete() + #this tells ggplot that the x-axis is categorical.
  geom_boxplot() + #creates a boxplot
  labs(y = "Time on Social Media") #short for "labels", use to label axes and titles.
```

::: {style="border-left: 4px solid #FF9800; background-color: #FFF3E0; padding: 10px; margin: 10px 0;"}
<strong>Warning:</strong> We receive a warning here because `ggplot()` is able to recognise and remove 'NA' values. Be **careful** as not all R functions are able to do this.
:::


::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What approximately is the median value? The lower quartile? The upper quartile? Is there another way that we could get this information in a more exact form ? Discuss this with your deskmate and your tutor.
:::

## Activity 2 - Creating a histogram in `ggplot()`

`ggplot()` can be customised with so many other functions that we have shown here to make truly [beautiful looking plots](https://r-graph-gallery.com/ggplot2-package.html). We will be learning how to do this throughout the next few weeks. 

For now lets see if you can put some of the skills you have learned so far to good use. See if you can work out how to make a histogram of the data using the function `geom_histogram()` 

::: {style="border-left: 4px solid #00bfa5; background-color: #e0f2f1; padding: 10px; margin: 10px 0;"}
<strong>Hint:</strong> You will only need to provide an x variable this time !
:::

```{r, eval=FALSE}
social_media_NA %>%
ggplot(aes(x = time_on_social)) + #ggplot uses aesthetic (aes()) to map axes. 
  geom_histogram() + #creates a histogram
  labs(x = "Time on social media", y = "Density") #short for "labels", use to label axes and titles.
```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What conclusions would you draw about the shape of the data, given your histogram? Please discuss with your deskmate and tutor.

## You are Free!

Well done ! You have completed everything you need to for this week. If you have finished in a record time please consult with your tutor about what to do next. Otherwise we will see you next lab ! 

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Students reaction to this information."}
knitr::include_graphics("https://en.meming.world/images/en/5/5a/Ight_Imma_Head_Out.jpg")
```





<!--chapter:end:Data_wrangling_and_visualisation.Rmd-->

# What is the sampling distribution of the mean

## Checking installation and loading packages

As usual we first always check and load in our required packages. 

```{r message = FALSE, warning= FALSE}
# Check if packages are installed, if not install.
if(!require(here)) install.packages('here') #checks if a package is installed and installs it if required.
if(!require(tidyverse)) install.packages('tidyverse')
if(!require(ggplot2)) install.packages('ggplot2')

library(here) #loads in the specified package
library(tidyverse)
library(ggplot2)
```

## Sampling distribution of the mean

This week we will be learning a lot about the sampling distribution of the mean. 

In last weeks lab you were introduced to a dataset looking at social media use in young adults. That data comes from a research programme run here at UNSW. However, this experiment has been repeated at 500 universities across the world, to get an in-depth global understanding of social media use in young adults.

Today you are going to look at your dataset from the last computing lab, and the data from across 500 universities. We will learn about the sampling distribution of the mean using the `time_on_social` variable.  Remember, that this variables stands for the following: 

-   `time_on_social` – average hours/day on social media (self-report diary)

## Checking the mean of `time_on_social` from last week

First we want to have another look at the `PSYC2001_social-media-data-cleaned.csv` dataset that we generated from last week. This dataset is 'clean' because we converted all `-999` values to `NA`. We can first load it in using the same`read.csv()` function combined with `here()`.

```{r}
social_media <- read.csv(file = here("Data","PSYC2001_social-media-data-cleaned.csv")) #reads in CSV files
```

Next we want to look at the mean of the variable `time_on_social`. This time since we have imported the clean version all we need to do is use the `summary()` function. Can you help with this ?
```{r finding the mean of time_on_social}

summary(social_media) #provides a summary of all variables in the data. 

```


::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What is the mean of `time_on_social` ? 
:::

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Deja Vu"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/What_is_the_sampling_distribution_of_the_mean/Deja%20Vu.jpg?raw=true")
```


Now we are going to start looking at the new dataset for this week. 

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> All the information about this data and its variables are located in the `README.txt` file. If you have **NOT** read this yet please make sure you do !
:::

## Activity 1 - Reading in Data

We are now going to read the dataset that we need for this week into R. The dataset can be read into an object called `global_social_media`. Please use the `read.csv()` and `here()` functions to read in the `PSYC2001_global-time-on-social-data.csv` file in the code block below. 

```{r error = TRUE}

#Use the read.csv() and here() functions to read in the dataset.

global_social_media <- read.csv(file = here("Data","PSYC2001_global-time-on-social-data.csv")) #your code goes here

```

## Checking the UNSW value in this dataset.

Now, lets check whether the UNSW value (reminder this is `U49` from the `README.txt` file!) matches the mean value we had from the first week. 

This should be pretty easy to do, and makes use of the `filter()` function we used [last week](#cleaning-the-data). This function is able to filter rows in your dataset that match a certain condition.

```{r}
global_social_media %>% 
  filter(uni_id == "U49") # reminder filter is used to select rows based on given conditions
```

Yay ! The output should match the mean of what we calculated last week. This is because each value in the new data file is the mean value for the `time_on_social` variable, for each of the 500 experiments run (U1-U500), i.e. the data contains 500 samples of the mean for `time_on_social`. This dataset is the result of repeating a single experiment, many times.



## Activity 2 - Finding the University of Sydney

Can you find the value for the University of Sydney (reminder this is `U102` from the `README.txt` file!) using the `filter()` function ? 

```{r}
global_social_media %>% 
  filter(uni_id == "U102")

```


## Vectors and the sample function

We are now going to have a look at what happens to the sampling distribution of the mean as we increase the number of mean samples in our samples (confusing I know !)

To do this we are going to make use of the `sample()` function from `baseR`. Lets first have a look at what this function does by using the `?` syntax. Please run the code block below:

```{r message = FALSE}
?sample
```

In essence the `sample()` function randomly selects values from a vector. Since this function takes a vector as its first argument we we cannot just give it the entire dataframe as it does not know what to do with it. This will result in an error. 

```{r giving sample the entire dataframe, error = TRUE}
sample(global_social_media, size = 10)

```

But you may be asking now, what is a vector? You can think of a vector as a single column in our dataframe. In this instance above, since the function only takes a single column, it gets overwhelmed when we pass it the entire dataframe. Poor function ! 

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Specter on a Vector"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/What_is_the_sampling_distribution_of_the_mean/Specter%20on%20a%20vector.jpg?raw=true")
```


So we need to pass it only a single column. We can do this by using the `$` operator from `baseR`. In essence the `$` operator extracts a specified column from our dataframe.

Lets have a go at using it below:

```{r Trying the $ operator}

head(global_social_media$uni_id) #head displays the first 6 elements. $ has been used to extract only the column uni_id

```

You can see that this has lead to only the column `uni_id` being extracted and shown in the function `head()`.

## What does the sampling distribution of the mean look like with only a few samples ? 

Great ! So we now know how to pass a vector (column) into the `sample()` function. First lets try extracting a low number of samples, say 20 samples.

```{r}
set.seed(1) #ensures we always get the same result from sampling ! 

mean_sample_20 <- sample(global_social_media$mean_time_on_social, size = 20) #randomly samples 20 data points from mean_time_on_social

mean_sample_20
```

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> You might notice we use the `set.seed()` function. The purpose of this is to create reproducible results from randomness. You can go [here](https://www.statology.org/set-seed-in-r/) if you want to know more
:::

This has extracted a sample of 20 means randomly from the `global_social_media` column `mean_time_on_social`. 

Now, what we want to do next is to visualise this data using a histogram. However, there is a problem.  The `sample()` function provides us with a single vector (column) but our plotting function `ggplot()` only likes dataframe. So we first need to convert our `mean_sample_20` vector into a dataframe. 

To do this we use the `data.frame()` function from `baseR` to create a dataframe.

```{r converting from a vector to a dataframe}
mean_sample_20_df <- data.frame(sample_20 = mean_sample_20) #create a dataframe with a column called sample_20 that takes values from our vector

head(mean_sample_20_df)

```

Now lets do some data visualisation ! We can create a histogram using `ggplot()` and the `geom_historgram()` functions from last week. We will be using a new function called `labs()` to label our x and y axis.

```{r, message = FALSE}

mean_sample_20_df %>% 
  ggplot(aes(x = sample_20)) +
  geom_histogram(fill = "skyblue", colour = "black") +  #fill and colour are Aesthetics. Fill controls the interior colour of shapes whereas colour controls the outline. 
  labs(x = "Time on Social media", y = "Count") #short for "labels", used to label axes and titles.

```

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> We are using some new aesthetics this week. We use the `fill` "skyblue" and the `colour` "black" to control the interior colour and border of our histogram respectively. You will learn more aesthetics that can be used to create nicer looking plots each week. 
:::

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What is the shape of the histogram here ? Is it as you expected ?
:::

## What does the sampling distribution of the mean look like as we add more samples? 

Now lets see what happens when we add in more samples. 

## Activity 3 - Increasing the sample size

Are you able to use the `sample()` function, `data.frame` function to create objects with 100, 250, 350 and 500 samples ? Use the code blocks below to do this. 
If you need any help please ask your tutor ! 

::: {style="border-left: 4px solid #00bfa5; background-color: #e0f2f1; padding: 10px; margin: 10px 0;"}
<strong>Hint:</strong> This is just replicating what we have done above with some new object names.
:::

```{r Create increasing sample sizes, error = TRUE}

#fill in the code below !

mean_sample_100 <- sample(global_social_media$mean_time_on_social, size = 100) #create a sampling distribution of the mean with 100 samples
  
mean_sample_250 <- sample(global_social_media$mean_time_on_social, size = 250) #create a sampling distribution of the mean with 250 samples
  
mean_sample_350 <- sample(global_social_media$mean_time_on_social, size = 350) #create a sampling distribution of the mean with 350 samples
  
mean_sample_500 <- sample(global_social_media$mean_time_on_social, size = 500) #create a sampling distribution of the mean with 500 samples

```

```{r have a try at converting to dataframes}

#fill in the code below ! 
mean_sample_100_df <- data.frame(sample_100 = mean_sample_100) #create a dataframe with a column called sample_100 that takes values from our vector

mean_sample_250_df <- data.frame(sample_250 = mean_sample_250) #create a dataframe with a column called sample_250 that takes values from our vector

mean_sample_350_df <- data.frame(sample_350 = mean_sample_350) #create a dataframe with a column called sample_350 that takes values from our vector

mean_sample_500_df <- data.frame(sample_500 = mean_sample_500) #create a dataframe with a column called sample_500 that takes values from our vector




```

Well done ! This was a hard activity. It you are struggling please ask your tutor for help.

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Ask for help !"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/What_is_the_sampling_distribution_of_the_mean/Ask%20for%20Help%20!.jpg?raw=true")
```

## Activity 4 - Visualising the sampling distribution of the mean with increasing samples 

Next we need to visualise all of these new samples. It is important we do this so we can see what happens to our sampling distribution of the mean as we increase the number of mean samples. 

We can do this by repeating the code for each histogram and giving it a new fill colour ! Are you able to help with this?  (Note there are of course much cleaner ways to do this, if you are interested please see the guide [here](https://www.datacamp.com/f/facets-ggplot-r?utm_source=google&utm_medium=paid_search&utm_campaignid=19589720824&utm_adgroupid=157156376351&utm_device=c&utm_keyword=&utm_matchtype=&utm_network=g&utm_adpostion=&utm_creative=733936254686&utm_targetid=aud-1832882613722:dsa-2218886984060&utm_loc_interest_ms=&utm_loc_physical_ms=9071851&utm_content=ps-other~apac-en~dsa~tofu~tutorial-r-programming&accountid=9624585688&utm_campaign=230119_1-ps-other~dsa~tofu_2-b2c_3-apac_4-prc_5-na_6-na_7-le_8-pdsh-go_9-nb-e_10-na_11-na&gad_source=1&gad_campaignid=19589720824&gbraid=0AAAAADQ9WsHmSJoqa8-5deMNCCGmJAmEp&gclid=CjwKCAjw9anCBhAWEiwAqBJ-c-0oZYPE5pP4Y6ZpYz0WabsrMLtJizQQn90osQAeAOfbaV9oUkleMxoClE4QAvD_BwE)).
```{r creating histograms for all samples, message = FALSE}

mean_sample_100_df %>% 
ggplot(aes(x = sample_100)) +
  geom_histogram(fill = "red", colour = "black") +
    labs(x = "Time on Social media", y = "Count")

mean_sample_250_df %>% 
ggplot(aes(x = sample_250)) +
  geom_histogram(fill = "blue", colour = "black") +
    labs(x = "Time on Social media", y = "Count")


mean_sample_350_df %>% 
ggplot(aes(x = sample_350)) +
  geom_histogram(fill = "green", colour = "black")+
    labs(x = "Time on Social media", y = "Count")


mean_sample_500_df %>% 
ggplot(aes(x = sample_500)) +
  geom_histogram(fill = "orange", colour = "black") +
    labs(x = "Time on Social media", y = "Count")



```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> How did the histogram change? Is it what you expected? Discuss this with your neighbours and your tutor.
:::

Now imagine that there were infinite universities that ran this experiment, that each collected a group of people’s `time_on_social` scores, and each gave us their sample mean value. That is the theoretical sampling distribution of the mean. 

The change in the shape of the histogram we have observed here is a critical implication of the central limit theorem - a large number of samples will lead to a approximately normal sampling distribution of the mean regardless of the actual population distribution. 

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Now thats all well and good but what does this actually mean ? Why do you think this actually matters for the statistics we do ? Discuss this with your neighbour and tutors.
:::

## Extension - What happens to a the sampling distribution of the mean for other population distributions ?

This section is an extension activity if you have already finished the required materials. Please check with your tutor that you have a good grasp of the material before moving onto this section.


```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Extension students be like"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/What_is_the_sampling_distribution_of_the_mean/Extension%20students.gif?raw=true")
```


Now lets get into it. Lets see what happens when we use an exponential population distribution and find its sampling distribution of the mean with a large number of samples. 

First, we are going to generate the population distribution. This can be done easily using the function `rexp()` which is used to generate exponential distributions. 

```{r}
# Generate an exponential population distribution
population <- data.frame(value = rexp(100000, rate = 1)) #generate an expotential distribution with 100,000 datapoints. 

```

Next lets generate a histogram of this population distribution and confirm that it looks like an exponential distribution.

```{r}
# Plot population distribution
ggplot(population, aes(x = value)) +
  geom_histogram( bins = 100, fill = "skyblue", color = "black") +
  labs(
       x = "Value",
       y = "Frequency") +
  theme_classic() #themes can be provided to ggplot which give it a bunch of aesthetics to change. One of these is theme_classic

```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Do you think this looks like an exponential distribution ? What should an exponential distribution look like ? Ask your tutor if you are not sure !
:::

Now we can take samples from this population. We use a new function here could `replicate()` which basically repeats the process inside the `{}` brackets a specified number of times. Inside the `{}` brackets is what we are actually doing population distribution. First we are taking a sample using the `sample()` function, then we are taking the mean of that sample using the `mean()` function. This will generate a similar set of data to `PSYC2001_global-time-on-social-data.csv`. That is, means of a bunch of samples from the population (i.e the sampling distirbution of the mean)

```{r}
# Lets take 500 samples from this population of size 50 per sample and calculate the mean. 
sample_means <- replicate(500, { #replicate the process 500 times
  sample_values <- sample(population$value, size = 50, replace = TRUE) # sample 50 values from the population mean
  mean(sample_values) #take the mean of those 50 sampled values
})


```

So what we have generated is a sampling distribution of the mean with 500 samples. Lets first convert that into a dataframe so that we can use `ggplot` to visualise this data.

```{r message = FALSE}

# Put results in a dataframe for plotting
sampling_df <- data.frame(sample_mean = sample_means) #convert the result into a dataframe

#plot the results
sampling_df %>% 
ggplot(aes(x = sample_mean)) +
  geom_histogram(fill = "skyblue", color = "black") +
  labs(x = "Sample Mean", y = "Frequency") +
  theme_classic()

```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What is the shape of this distribution ? is it different to the population distribution from above ? What are the implications of this ?
:::

## You are Free!
  
Well done you have completed another computing tutorial. This one has been very difficult and you have done a terrific job. See you all next week for more computing fun ! 

```{r, echo=FALSE, out.width="75%", fig.align='center', fig.cap="Celebrating finishing this tutorial !"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/What_is_the_sampling_distribution_of_the_mean/Crab%20celebration.gif?raw=true")
```




<!--chapter:end:What_is_the_sampling_distribution_of_the_mean.Rmd-->

# Testing our first hypothesis

Today we are going to ask our first question and seek an answer from the data. We will get the data into shape so that it is in the right format for visualizing and analysing. Then we will run the analysis and learn the answer to our question. Welcome. You are now a psychologist :)


## Checking installation and loading packages

As usual we first always check and load in our required packages.

```{r message = FALSE, warning= FALSE}
# Check if packages are installed, if not install.
if(!require(here)) install.packages('here') #checks if a package is installed and installs it if required.
if(!require(tidyverse)) install.packages('tidyverse')
if(!require(ggplot2)) install.packages('ggplot2')

library(here) #loads in the specified package
library(tidyverse)
library(ggplot2)
```


## Developing our hypotheses

Today we are going to address one of the key questions of the study about social media use – how does mood affect active social media use? Active social media use involves interacting with content (i.e. liking posts) rather than just observing posts.

There is some evidence to suggest that passive social media use is associated with lower mood in adolescents, whereas active social media use is related to positive mood [Dienlin & Johannes, 2020](https://pmc.ncbi.nlm.nih.gov/articles/PMC7366938/). However, a lot of the existing evidence comes from self-report, rather than measuring social media behaviour directly.

To address this question, the researchers used the participants metadata to count how frequently they liked posts. By cross-referencing this with the mood diary kept by each participant, they were able to calculate the average number of likes per 10 minutes of use when participants were in a good mood, and when they were in a bad mood.

It should be clear by now that we will be addressing this question using the `good_mood_likes` and `bad_mood_likes` varaibles.  Remember, these variables stand for the following:

-   `good_mood_likes` – average number of likes made over 10 min during a good mood (from platform + diary)
-   `bad_mood_likes` – as above, but during bad mood


## Activity 1 - Defining our hypotheses

Based on the information above, discuss and formulate hypotheses around the following:

1. Should there be a difference in the number of likes between the mood conditions
2. What direction do you think this difference could be? Can you formulate an experimental hypothesis each way – i.e. good mood likes > bad mood likes, and vice versa?
3. What is the null hypothesis?

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Discuss this with your neighbour and your tutor. Make sure you have clearly defined your hypotheses before moving forward.
:::

## Visualising our data

First we want to load in the dataset `PSYC2001_social-media-data-cleaned.csv` dataset.  To do this we use the same `read.csv()` function combined with `here()`. Do you remember how to do this ? 

```{r }
social_media <- read.csv(file = here("Data","PSYC2001_social-media-data-cleaned.csv")) #reads in CSV files
```


Before we conduct any kind of statistical test it is a good idea to see how it looks. This will give us an understanding about the underlying data that is driving the results of our statistical test. 

::: {style="border-left: 4px solid #FF9800; background-color: #FFF3E0; padding: 10px; margin: 10px 0;"}
<strong>Warning:</strong> It is generally **bad practice** to go straight from the raw data to the results of a statistical test without first visualising the data.
:::



```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Professors reaction when you don't visualise data"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Testing_our_first_hypothesis/Statistics%20professors%20reaction.gif?raw=true")
```


## Density plot of the distribution

We can look at the distribution of the data using a density plot. A density plot is a smoothed version of a histogram which allows us to understand what the full distribution might look like if we had all the data in the world. More information on that [is here](https://www.data-to-viz.com/graph/density.html) if you are interested !

In order to make this easy to plot we first have to wrangle our data a bit. What we want to do here is convert our data from a wide-format (which is more digestible for us mere humans) into a long format (which programs like R generally like more!). More on that [here](https://www.statology.org/long-vs-wide-data/).

First we use the `select()` function which allows us to easily choose which columns we do (or don't) want to keep in our dataframe. Second we use the `pivot_longer()` function which allows us to combine different columns together. Both of these functions come from the `tidyverse` package. In the code below we take the columns ending with "likes" and put their values (i.e the number of likes) into the column "likes" and their column names names (i.e good or bad mood) into the column "mood". 

```{r Pivoting data}
social_media_likes <- social_media %>% 
  select("id","good_mood_likes","bad_mood_likes") %>% # choose which columns we want keep in our dataframe
  pivot_longer(cols = ends_with("likes"), names_to = "mood", values_to = "likes") #take columns ending with "likes" and move the column names into "mood" and column values into "likes"

```

Let's make sure that this data still looks okay by using the `head()` function.

```{r View of the data}
head(social_media_likes)
```


Now, we can get a density plot in R by using the `geom_density` function inside `ggplot()`.


```{r Create a density plot, message = FALSE}

social_media_likes %>% 
ggplot(aes(x = likes, colour = mood)) +
  geom_density(linewidth = 2) +
    labs(x = "Number of likes", y = "Density") +
  theme_classic() #themes can be provided to ggplot which give it a bunch of aesthetics to change. One of these is theme_classic
```

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> We are using a new function here called `theme_classic()`. This makes the graph look at a bit nicer. There are a bunch of other available themes [here](https://ggplot2.tidyverse.org/reference/ggtheme.html). We also have a new argument `colour` inside the `aes()` function which tells `ggplot()` to use different line colours depending on the participants mood (i.e good or bad mood).
:::


## Activity 2 - Choosing a new theme

Lets have a go at replicating the previous graph but using a new theme. You can pick whichever theme you think looks the best. Notice that changing the theme will only change non-data related aesthetic features of the graph.


```{r Activity 2 - Changing ggplot themes, message = FALSE}

social_media_likes %>% 
ggplot(aes(x = likes, colour = mood)) +
  geom_density(linewidth = 2) +
    labs(x = "Number of likes", y = "Density") +
  theme_bw() #themes can be provided to ggplot which give it a bunch of aesthetics to change. One of these is theme_classic
```

## Descriptive statistics

Before we can move onto conducting t-tests it is always a good idea to generate our descriptive statistics. For the data we are looking at **the most relevant descriptive statistics are the mean and standard deviation**. This is because we want to conduct a t-test to compare the average likes in different moods. We can easily get this information in R by using the  `summarise()` function together with the `group_by()` function. The `group_by()` splits the data into different groups and the `summarise()` function calculates a statistic (i.e the mean and standard deviation). Both of these packages come from the `tidyverse` package.

```{r}
social_media_descriptives <- social_media_likes %>% 
  group_by(mood) %>% #split the data by mood
  summarise(mean_likes = mean(likes),
            sd_likes = sd(likes)) #calculate the mean number of likes

head(social_media_descriptives)
```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What are the mean number of likes in each different mood ? What is the standard deviation ?
:::

## Activity 3 - Calculating the median number of likes

Great ! We have the mean number of likes when subjects are in a good and bad mood. Please see if you are able to use these functions to instead calculate the median number of likes under different mood conditions. 

<div style="border-left: 4px solid #00bfa5; background-color: #e0f2f1; padding: 10px; margin: 10px 0;">
  <strong>Hint:</strong> The `median()` function is used to calculate medians ! </div>

```{r}
social_media_median <- social_media_likes %>% 
  group_by(mood) %>% #split the data by mood
  summarise(median_likes = median(likes)) #calculate the mean number of likes

head(social_media_median)

```

## Testing hypotheses manually

Now that we have had a look at the structure and descriptives statistics of our data we can have a go at using a t-test to compare the number of likes participants made in a good and bad mood. We could of course do this manually, by hand (as we have in the statistics tutorials). We have shown this below using the first 10 values for good and bad mood likes. 

```{r, echo=FALSE, out.width="75%", fig.align='center', fig.cap="t-test table"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Testing_our_first_hypothesis/Manual%20t-test%20table.JPG?raw=true")
```

Thankfully, we have long since past the stone (pen) age so it is no longer necessary to do this by hand. We can get computers to do this for us! 

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Handwritten statistiscs be like"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Testing_our_first_hypothesis/Space%20odyssey.gif?raw=true
")
```

## Testing hypothesis using a two-sample t-test

We can very easily calculate a t-test in R using the `t.test()` function which comes as part of `baseR`. Please first have a look at this function using the `?` syntax to understand its arguments. We recommend using the console from now on to do this rather than creating a codeblock. 

```{r, echo=FALSE, out.width="100%", fig.align='center', fig.cap="Finding the Console"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Testing_our_first_hypothesis/Navigating%20to%20the%20Console.gif?raw=true")
```

We can see that `t.test()` takes different arguments which are important for the way it handles the data. What values you provide to these arguments is dependent on what kind of test you want to conduct. One of the most important arguments to fill in is the `paired` argument. This determines whether you are conducting an independent or paired samples t-test. Here, we have collected the number of likes for the same subject in different moods. This means this data comes from a repeated-measures design, and so a paired t-test is most appropriate. We also need to provide `t.test()` with a formula telling it what to do with our data. The left hand side is the numerical variable and the right hand side is a grouping variable.


```{r conducting our t-test}
t.test(formula = likes ~ mood, data = social_media_likes, paired = TRUE)  #compute a t-test. Likes ~ mood indicates that likes is the numerical variable and mood is a grouping variables. Paired indicates whether to conduct a paired or independent t-test.
```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Discuss the output of this paired t-test with your neighbour and tutors. What does it tell us ?
:::

## Activity 4 - One-sample t-tests

Now that we have the output of a paired t-test we can compare it to a one sample t-test. The results of both of these tests should be identical (this has been covered in the lectures). For those that haven't watched the lectures, this is because a paired t-test is basically a one-sample t-test of matched difference scores ([more on that here](https://statisticsbyjim.com/hypothesis-testing/t-tests-1-sample-2-sample-paired-t-tests/))

Are you able to conduct a one-sample t-test using R ? 

First you will need to use the `mutate()` function on the original `social_media` dataframe to create a difference column.
```{r Activity 3 - First create a new column}
social_media_diff <- social_media %>% 
  mutate(likes_diff = good_mood_likes - bad_mood_likes) #first try to create a new column which is the difference between mood likes

head(social_media_diff)
```

Once we have correctly calculated our difference score we can use the `t.test()` function to get our results. 

```{r Activity 3 - Now conduct a one-sample t-test}

t.test(formula = likes_diff~1, data = social_media_diff) #the formula likes_diff~1 indicates that there is no grouping variables (i.e just compare to 0!)
```

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> The right hand side of the formula 'likes_diff~**1**' indicates that there is no grouping variable.
:::

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Compare the output of the one-sample t-test and the paired t-test. Are they the same ?
:::

## Writing up results and conclusions

Let's have a go at writing up the results of the paired samples t-test we have conducted in APA format. While it is not necessarily critical to use APA formatting outside of these psychology courses, it is important to follow some existing format so that readers can easily interpret your results and conclusions about important data.

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Results:</strong> A paired samples t-test indicated that the number of likes was greater when participants were in a bad mood (M = 49.8 , SD = 17.2) than in a good mood (M = 43.0 , SD = 16.1) and this difference was statistically significant (t(59) = 3.336, p = 0.001).
:::

What can we conclude from this? Please have a go at writing this up. 

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Conclusions:</strong> Your conclusion will go here !
:::


## Plotting our results

Now that we have conducted our statistical test we need to present the results in the simplest way possible. One way to do this is by using the average number of likes when participants are in a good and bad mood and plotting it using a column graph.


## Activity 5 - Plotting the mean number of likes

Are you able to plot the mean number of likes as a column graph ?

::: {style="border-left: 4px solid #00bfa5; background-color: #e0f2f1; padding: 10px; margin: 10px 0;"}
<strong>Hint:</strong> This will be very similar to the code used to plot the density graph but instead using `geom_col()`.
:::


```{r Activity 5}

social_media_descriptives %>%   #write code that produces a column graph of likes with a different colour for each mood
ggplot(aes(y = mean_likes, x = mood, fill = mood)) +
  geom_col(linewidth = 2) +
    labs(y = "Number of likes", x = "Mood")+
  theme_classic()

```

## You are Free!

Well done you have finished for this week! There is no extension activity this week so if you have finished please confirm that you understand everything with your tutor.

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Students leaving evil statistics classes"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Testing_our_first_hypothesis/Students%20leaving.gif?raw=true")
```




<!--chapter:end:Testing_our_first_hypothesis.Rmd-->

# Testing between groups

## Checking installation and loading packages

As usual we first always check and load in our required packages.

```{r  message = FALSE, warning= FALSE}
# Check if packages are installed, if not install.
if(!require(here)) install.packages('here') #checks if a package is installed and installs it if required.
if(!require(tidyverse)) install.packages('tidyverse')
if(!require(ggplot2)) install.packages('ggplot2')

library(here) #loads in the specified package
library(tidyverse)
library(ggplot2)
```

## Testing between groups

We observed last week how mood impacts active social media behaviour. However, that is not the only factor that influences social media use. For example, [Sapienza et al (2023)](https://academic.oup.com/pnasnexus/article/2/11/pgad357/7442564) found that people in rural areas are more likely to use their smartphone for social media and gaming, whereas urban dwellers are more likely to use their phone for navigation and business.

However, we do not know if people living in urban and rural areas engage with social media differently, regardless of how long they spend on their chosen platforms. Today we will address this question using the `urban`, `good_mood_likes`, `bad_mood_likes`, and `followers` variables. Remember, these variables stand for the following:

-   `urban` – urban (1) or rural (2) area (based on postcode density)
-   `good_mood_likes` – average number of likes made over 10 min during a good mood (from platform + diary)
-   `bad_mood_likes` – as above, but during bad mood
-   `followers` – average number of followers across platforms


## Activity 1 - Formulate your research question

What do you think? Will urban and rural dwellers engage differently with social media? Will there be a difference in the number of likes made by people living in urban vs rural areas? Or in the number of followers people have in urban vs rural areas?

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What are the null hypotheses for your research questions (you should have one for ‘likes’ and one for ‘followers’? What would you expect to see if your prediction is correct? Discuss this with your neighbor and your tutor.
:::

::: {style="border-left: 4px solid #00bfa5; background-color: #e0f2f1; padding: 10px; margin: 10px 0;"}
<strong>Hint:</strong> We are going to average over the effect of mood, so we do not need to include mood in our predictions about likes.
:::

## Activity 2 - Creating our likes variable

Today we will be averaging across mood to get the number of likes for urban and rural dwellers. This means we first need to create a new variable called `likes` which is the average of the likes in a good and bad mood.

We first load in our`PSYC2001_social-media-data.csv` dataset.

```{r }
social_media <- read.csv(file = here("Data","PSYC2001_social-media-data-cleaned.csv")) #reads in CSV files
```

Lets double check its the data we think it is...

```{r check data}

head(social_media, 10) # you can even say how many lines you want to see! Try changing the number, and see what happens.
```

Are you able to fill the code below using the `mutate()` function to create this new 'likes' variable? 

::: {style="border-left: 4px solid #00bfa5; background-color: #e0f2f1; padding: 10px; margin: 10px 0;"}
<strong>Hint:</strong> The average of likes is `(good_mood_likes + bad_mood_likes)/2`
:::

```{r }

social_media_likes <- social_media %>% 
  mutate(likes =(bad_mood_likes + good_mood_likes)/2 ) %>% #create a new column with specified values
  select(id, urban, likes, followers) #keep specified columns in dataframe

head(social_media_likes)

```


## Wrangling our data

Now that we have this object it is important to check the format of the data. Lets use the `str()` function that we learned about in the [second tutorial](#importing-your-data-in-r) to do this.

```{r}
str(social_media_likes) #provides a summary of the data structure.
```

First we can see that having the values in `urban` are coded as either 1 (urban) or 2 (rural). Lets change this so that instead of using numbers we use the actual descriptions of urban and rural. It is surprisingly difficult to remember what 1 and 2 stand for once you come to look at your graphs. Life will be much easier if we can see what 1 and 2 stand for instead. To do this we will use the `mutate()` function with the `case_when()` function which replaces (or creates) specific values in a variable with new ones.

```{r}
social_media_likes <- social_media_likes %>% 
  mutate(urban = case_when(urban == 1 ~ "urban", 
                           urban == 2 ~ "rural")) #case_when uses if_else logic to replace values with specified values if the cases match.

str(social_media_likes)

```

We can see that `urban` is now classed as a `chr` (character) but we will eventually need to split our graphs by `urban`. This means that `urban` should be a factor instead. (Also, urban is a factor in the experiment, so lets keep it consistent). Luckily, we can change this easily by using `as.factor()` within the `mutate()` function. The `as.factor()` function is used to convert other datatypes to factors !

```{r}
social_media_likes <- social_media_likes %>% 
  mutate(urban = as.factor(urban))

str(social_media_likes)
```

The data is now in a format that we should be able to easily visualise it and conduct our statistical tests. Well done !

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="What it feels like teaching this section"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Testing_between_groups/What%20it%20feels%20like%20to%20explain%20this%20section.gif?raw=true")
```


## Activity 3 - Visualising our data before analysis 

We’re now going to look at the data in 2 ways. First, we’re going to look at how the data is distributed across all participants, so that we can check if the data meets our assumptions about normality. Second, we are going to plot our dependent variables (`likes`, `followers`) by group, to gain a visual understanding for what group differences might look like, if they exist.

Here, we create a density plot for `likes`. 

```{r Density plot of likes}
social_media_likes %>% 
  ggplot(aes(x = likes)) +
  geom_density(linewidth = 2, colour = "blue") + #the argument linewidth is used to alter the size of the density line. 
  labs(x = "Number of Likes", y = "Density") +
  theme_classic() 
```

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> We use a new argument here `linewidth` to control the size of the density line.
:::


Now, can you make a density plot for `followers`? Go on, you are a coder and you are strong.

```{r Density plot of followers}

social_media_likes %>% 
  ggplot(aes(x = followers)) + 
  geom_density(linewidth = 2, colour = "orange") +
  labs(x = "Number of followers", y = "Density") +
  theme_classic() 

```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Do `likes` and `followers` look normally distributed to you? Why might the data be shaped how they are for each variable?
:::

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What impact does your new understanding of the data have on your analysis, if any?
:::

## Activity 4 - Visualising group differences

Now we are going to make some density plots and boxplots, split by the urban factor so that we can see the group differences.

Are you able to help with this?

::: {style="border-left: 4px solid #00bfa5; background-color: #e0f2f1; padding: 10px; margin: 10px 0;"}
<strong>Hint:</strong> Use the `colour` argument in `aes()` to split the plot by `Urban`
:::

```{r Density plot likes split by urban}
social_media_likes %>% 
  ggplot(aes(x = likes, colour = urban)) +
  geom_density(linewidth = 2) +
  labs(x = "Number of Likes", y = "Density") +
  scale_colour_manual(values = c(rural = "purple", urban = "green")) + #manually define colours of specific parts of a graph
  theme_classic() 
```

```{r Density plot followers split by urban}
social_media_likes %>% 
  ggplot(aes(x = followers, colour = urban)) +
  geom_density(linewidth = 2) +
  labs(x = "Number of Followers", y = "Density") +
  scale_colour_manual(values = c(rural = "purple", urban = "green")) + #manually define colours of specific parts of a graph
  theme_classic() 
```

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> We are using a new function here called `scale_colour_manual()`. This allows you manually define the colours of specific parts of a graph. Here we have used it to define colours for specific groups.
:::

This is not the only way we may want to view the group differences in likes and followers. Using a grouped boxplot can be a handy way at looking at how groups differ from each other.

```{r Boxplot of likes split by urban}
social_media_likes %>% 
  ggplot(aes(y = likes, colour = urban, x = urban)) +
  geom_boxplot() +
  labs(x = " ", y = "Number of Likes") +
  scale_colour_manual(values = c(rural = "purple", urban = "green")) + #manually define colours of specific parts of a graph
  theme_classic() 

```

Can you change the code below so that the x-axis has a label, and the y-axis label is correct?

```{r Boxplot of followers split by urban}
social_media_likes %>% 
  ggplot(aes(y = followers, colour = urban, x = urban)) +
  geom_boxplot() +
  labs(x = "", y = "Number of Likes") +
  scale_colour_manual(values = c(rural = "purple", urban = "green")) + #manually define colours of specific parts of a graph
  theme_classic() 

```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What do you think the data suggest about the group differences for `likes` and `followers`? Was it in-line with your predictions from activity 1? Are there any caveats or reasons to be cautious about your interpretations?
:::

## Activity 5 - Descriptive statistics

It is important to generate descriptive statistics before our t-test. Are you able to fill in the code below to generate the mean and standard deviation of likes and followers for urban and rural dwellers ? 

```{r}
social_media_descriptives <- social_media_likes %>% 
  group_by(urban) %>% 
  summarise(
    mean_followers = mean(followers),
    mean_likes = mean(likes),
    sd_follower = sd(followers),
    sd_likes = sd(likes)
  )

head(social_media_descriptives)
```

## Independent samples t-test

We now want to learn whether we have evidence for differences between urban and rural dwellers on the 'likes' and 'followers' variables.

## Activity 6 - Undertaking an independent samples t-test

Can you work out how to perform an independent samples t-test for these variables ?

::: {style="border-left: 4px solid #00bfa5; background-color: #e0f2f1; padding: 10px; margin: 10px 0;"}
<strong>Hint:</strong> Use the `t.test()` function from the tutorial last week and pay attention to the 'paired' argument !
:::

```{r}
t.test(formula = likes ~ urban, data = social_media_likes, var.equal = TRUE, paired = FALSE)
```

```{r}
t.test(formula = followers ~ urban, data = social_media_likes, var.equal = TRUE, paired = FALSE)
```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Discuss the output of this independent t-tests, what does it tell you about the differences between urban and rural dwellers and how they use social media ? Is it what you expected when you formulated your hypotheses?
:::

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Exams are hard"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Testing_between_groups/Exams%20are%20hard.jpg?raw=true")
```


## Writing up results and conclusions

Lets have a go at writing up the results and conclusions for this independent samples t-test.

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Results:</strong> An independent samples t-test indicated that the number of likes was greater for rural (M = 52.1 , SD = 12.7) compared to urban dwellers  (M = 40.8 , SD = 14.5) and this difference was statistically significant (t(58) = 3.22, p = 0.002). Conversely, the number of followers was greater for urban (Mean = 143.9, SD = 62.0) than for urban dwellers (M = 105.6, SD = 40.9) which was statistically significant (t(58) = 2.82, p = 0.007).
:::


Given these results have a go at writing up our conclusions. 

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Conclusions:</strong> Your conclusion will go here !
:::

## You are Free!

Well done ! This computing tutorial is now over. Make sure to thank your tutor for another amazing class full of wonderful statistics and learning !

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Everyone loves statistics ?"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Testing_between_groups/Everyone%20loves%20statistics.jpg?raw=true")
```

<!--chapter:end:Testing_between_groups.Rmd-->

# You got the power !

This week, we are going to explore how improving our statistical power changes our ability to detect effects, if they exist.

## Checking installation and loading packages

As usual we first always check and load in our required packages.

```{r message = FALSE, warning= FALSE}
# Check if packages are installed, if not install.
if(!require(here)) install.packages('here') #checks if a package is installed and installs it if required.
if(!require(tidyverse)) install.packages('tidyverse')
if(!require(ggplot2)) install.packages('ggplot2')

library(here) #loads in the specified package
library(tidyverse)
library(ggplot2)
```

## Key power facts:

1. We can never know for sure what size our effect is going to be – unless we measured the entire population of interest – which is quite often THE ENTIRE HUMAN SPECIES. That would be a HUUUUUGE experiment. This means we have to make a theoretical or educated guess, and power our study accordingly.

2. The effect size we are hunting depends on the nature of the effect, not the size experiment we want to run. For example, the effect of getting the right nutrients on height is the same, regardless of how many people we sample in our experiment.

**Therefore**: our job as experimenters is to make the most educated guess that we can about the effect size we are looking for, and make sure our study is appropriately powered to give us the best chance to detect it, **if it even exists.**

The **best chance we have to change statistical power is to change our sample size.**

## Simulating sample sizes

In this first exercise we are going to simulate the impact of sample size on our experiment results using the `PSYC2001_global-time-on-social-data.csv`. We will learn about influence of sample size on **effect size and p value** using the `mean_time_on_social` variable.  Remember, that this variables stands for the following: 

-   `mean_time_on_social` – average hours/day on social media per experiment (self-report diary)

## Installing and loading new packages we will be using today

We will be using two new packages today. The `pwr` package to complete power calculations and the `effsize` package to generate effect sizes. Please run the code to install and load these packages.

```{r message = FALSE, warning= FALSE}
if(!require(effsize)) install.packages('effsize')
if(!require(pwr)) install.packages("pwr")

library(pwr)
library(effsize)
```


## Activity 1 - Simulating data 

Before we can simulate data we first need to load in our dataset. Can you help with this ? 

```{r}

#Use the read.csv() and here() functions to read in the dataset.

global_social_media <- read.csv(file = here("Data","PSYC2001_global-time-on-social-data.csv"))

```

To simulate our data we will be using our friend the `sample()` function from the previous tutorial. If you need a reminder on the usage of this function please click [here](#vectors-and-the-sample-function) or use the `?` syntax. 

Now that you are an expert on this function are you able to sample data for n = 10, n= 30, n= 50, n = 100, n= 200 and n = 500 ? Have a try at filling in the code block below:

```{r}
set.seed(1) #ensures we always get the same result from sampling ! 

sample_10 <- sample(global_social_media$mean_time_on_social, size = 10)
sample_30 <-sample(global_social_media$mean_time_on_social, size = 30)
sample_50 <-sample(global_social_media$mean_time_on_social, size = 50)
sample_100 <-sample(global_social_media$mean_time_on_social, size = 100)
sample_200 <-sample(global_social_media$mean_time_on_social, size = 200)
sample_500 <-sample(global_social_media$mean_time_on_social, size = 500)

```

Now that we have got our simulated data we are going to test whether the sample of `time-on-social` data is on average, above the international average of 2.2 hours. To do this we can run a one sample t-test against the test value of 2.2 for each of our simulated datasets. This will return a **p value** that we can use later on. We also want to get the **effect size** for each of these t-tests. To do this we will also using the `cohen.d()` function from the `effsize` package. It has the same arguments as the `t.test()` function as you will see below. 

## Activity 2 - Conducting t tests and effect sizes. 

Are you able to fill in the code below to help with this ? 

```{r}
t_test_10 <- t.test(sample_10 ~ 1, mu = 2.2)
eff_size_10 <- cohen.d(sample_10 ~ 1, mu = 2.2)

t_test_30 <- t.test(sample_30 ~ 1, mu = 2.2)
eff_size_30 <- cohen.d(sample_30 ~ 1, mu = 2.2)

t_test_50 <- t.test(sample_50 ~ 1, mu = 2.2)
eff_size_50 <- cohen.d(sample_50 ~ 1, mu = 2.2)

t_test_100 <- t.test(sample_100 ~ 1, mu = 2.2)
eff_size_100 <- cohen.d(sample_100 ~ 1, mu = 2.2)

t_test_200 <- t.test(sample_200 ~ 1, mu = 2.2)
eff_size_200 <- cohen.d(sample_200 ~ 1, mu = 2.2)

t_test_500 <- t.test(sample_500 ~ 1, mu = 2.2)
eff_size_500 <- cohen.d(sample_500 ~ 1, mu = 2.2)
```

At this point it is worth knowing that running the `t.test()` function returns a lot of different things. Run the code block below to have a look. 

```{r}
view(t_test_10)
```

The same thing applies to running the `cohen.d()` function. Lets have a look at what it returns.

```{r}
view(eff_size_10)
```

In this case we only want the `p-value` from the `t.test()` output and the `estimate` (cohens.d) from the `cohen.d()` output. We can extract these by using the `$` syntax to pull the `p-value` and `estimate`.

Please try to help with this below:

```{r}
p_value_10 <- t_test_10$p.value
p_value_30 <- t_test_30$p.value
p_value_50 <- t_test_50$p.value
p_value_100 <- t_test_100$p.value
p_value_200 <- t_test_200$p.value
p_value_500 <- t_test_500$p.value

cohen_d_10 <- eff_size_10$estimate
cohen_d_30 <- eff_size_30$estimate
cohen_d_50 <- eff_size_50$estimate
cohen_d_100 <- eff_size_100$estimate
cohen_d_200 <- eff_size_200$estimate
cohen_d_500 <- eff_size_500$estimate
```

Well done ! This was really difficult and you have done a great job simulating this data. We now have a **p-value** and **effect size** for our different sample sizes. 

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="Crappy code be like"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/You_got_the_power/Terrible%20code.gif?raw=true")
```


::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong>There is a lot of repeated code here. Good coding practice generally suggests to reduce the amount of repeated code to improve the readability and efficiency of your code (not to mention how long it takes to write !). One way to do this is by using the `sapply()` function which loops or repeats functions (or functions of functions !). For more on that please go [here](https://nicercode.github.io/guides/repeating-things/).
:::

## Activity 3 - Plotting our simulated data

Now let's see what we can learn from our simulation results. First lets combine our **p-values** and **effect sizes** into a dataframe so it can be plotted. The simplest approach is to start by organising the results into three vectors using the `c()` syntax:

1. **p-values vector** – stores all of the p-values in order.

2. **effect sizes vector** – stores the corresponding effect sizes in the same order.

3. **sample sizes vector** – stores the number of samples for each test, also in the same order.

Each position across these three vectors matches up. For example, the first entry in all three vectors comes from the same test, the second entry from the next test, and so on. The sample size vector acts like a label, letting you track which **p-value** and **effect size** came from which sample size. 

```{r}
sample_size <- c(10,20,30,100,200,500) #the c() syntax concatenates values into a vector
p_values <- c(p_value_10,p_value_30,p_value_50,p_value_100,p_value_200,p_value_500)
cohens_ds <- c(cohen_d_10,cohen_d_30,cohen_d_50,cohen_d_100,cohen_d_200,cohen_d_500)
```

We can now combine these into a single dataframe using the `data.frame()` function. Note we explicitly use the `factor()` function here to make sure that R knows that our sample size is a factor not a numeric variable. 

```{r}
samples_df <-data.frame(sample_size = factor(sample_size), #here we make sure to explicitly tell R that sample_size is a factor and not a numeric (which would result in weird things!)
              p_values = p_values,
              cohens_ds = cohens_ds)


```

Finally, are you able to use your growing `ggplot` prowess to plot how the **p-value** and **effect size** changes across sample sizes ? 

```{r}
samples_df %>% 
  ggplot(aes(x = sample_size, y = p_values)) +
  geom_col(fill = "blue") +
  labs(x = "Sample Size", y = "p value") +
  theme_classic()


```

```{r}
samples_df %>% 
  ggplot(aes(x = sample_size, y = cohens_ds)) +
  geom_col(fill = "red") +
  labs(x = "Sample Size", y = "p value") +
  theme_classic()
```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> How do the effect sizes change across N, and why might this be? How do the p-values change across N, and why might this be? What have you learned about using statistical power in experiment design, after 1 & 2?
:::

## Power analysis in R

Now that we have figured out that the **main purpose of a power analysis is to calculate the sample size required to detect an effect of interest** let's see how we can do this easily in R.

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="The power of power analyses"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/You_got_the_power/The%20power%20of%20power%20analyses.jpg?raw=true")
```


## Power analysis on correlations

First let's figure out how to do power analysis on correlations. When conducting a power analysis for correlations (as should be clear from your lectures and tutorials on power analysis) there are a few important ingredients. 

1.  **Desired power**. Lets say we want 90% power to detect our effect.
2.  **Alpha level**. This is conventionally set to 0.05. 
3.  **Correlation coefficient 'r'**. Lets say that we are looking for a correlation of r = 0.5
  
Now that we have these parameters we can perform a power calculation using the `pwr.r.test` function. 
  
```{r}
pwr.r.test(r = 0.50, sig.level = 0.05, power = 0.9)
```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What is the takeaway from this output ? What sample size is required to detect our effect of interest?
:::

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> We can also use this package to do power calculations for other tests like t-tests and regressions.
:::


## Activity 4 - Conducting a power analysis for a t-test

We can also use the power analysis functions from the `pwr` package to perform power analyses for other types of statistical tests. For instance, we can conduct a power analysis for an independent samples t-test using the function `pwr.t.test`. 

When performing a power analysis on a t-test the ingredients required are mostly the same.

1.  **Desired power**. Lets say we want 90% power to detect our effect.
2.  **Alpha level**. This is conventionally set to 0.05. 
3.  **Effect size** . Lets say that we are looking for an effect size of d = 0.5.

Please fill in the code below to determine the sample size. 

```{r}
pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.9, type = "two.sample")
```

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> You will notice that instead of using gamma ($\gamma$) like we do for handwritten calculations we use cohens d in the `pwr` package. Both measures are valid measures of the size of effect and can be used in power calculations. 
:::


## Writing up a power analysis

Let's have a go at writing out the results and conclusions from this power analysis

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Results:</strong> A power analysis was performed to determine the minimal sample size required to detect a medium effect size with 90% power. Results indicated that the minimum sample size required to achieve 90% power for detecting a medium effect size (d = 0.50) at a significance level of alpha = .05 (two tailed) was 86 participantsper group for an independent samples t-test.
:::

Now have a go at writing the conclusion yourself. You got this ! 

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Conclusions:</strong> Your conclusion will go here !
:::

## You are Free!

Well done finishing this tutorial. You've got so much power now. We will see you all next week for more computing fun !

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="The power of power analyses"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/You_got_the_power/Power.jpg?raw=true")
```


<!--chapter:end:You_got_the_power!.Rmd-->

# Correlation in R

## Checking installation and loading packages

As usual we first always check and load in our required packages.

```{r message = FALSE, warning= FALSE}
# Check if packages are installed, if not install.
if(!require(here)) install.packages('here') #checks if a package is installed and installs it if required.
if(!require(tidyverse)) install.packages('tidyverse')
if(!require(ggplot2)) install.packages('ggplot2')

library(here) #loads in the specified package
library(tidyverse)
library(ggplot2)

```


## Investigating correlational relationships

This week, we are going to explore relationships between continuous variables, using correlation.

We have previously looked at how mood and location influence social media use. Now we’re going to look at the relationship between social media use, age and political activism. It has previously been shown that social media use has a positive relationship with levels of political activism in other countries such as Jordan [(Alodat et al, 2023)](https://www.mdpi.com/2076-0760/12/7/402). However, we don’t know if this is true for young adults in Australia. This is what we will test today.

Today we will be using the `time_on_social`, `age`, `polit_informed`, `polit_campaign` and `polit_activism` variables.

Remember, these variables stand for the following:

- age – age in years
- time_on_social – average hours/day on social media (self-report diary)

Political attitude subscales:

- informed – how politically informed they feel (e.g., read news daily)
- campaign – how much they engage in campaign-related discussion
- activism – involvement in activism (e.g., protests, petitions)

## Activity 1 - Formulate your research question

Before we begin we first need to load in our dataset for today. Are you able to load in the `PSYC2001_social-media-data.csv` dataset ? 

```{r }
social_media <- read.csv(file = here("Data","PSYC2001_social-media-data-cleaned.csv")) #reads in CSV files
```


Will young adults who spend a lot of time on social media be more politically active ? Or the opposite? Or neither ? 


::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What is a reasonable null and alternate hypothesis? The class will come up with a hypothesis.
:::

## Activity 2 - How much political attitude?

Let's now get an idea of our much political attitude these participants have. We have scores from 3 subscales of a political-attitudes questionnaire. We need to combine them into a single ‘attitude’ score for each participant. Each subscale score carries a different weighting towards the total ‘attitude’ score.

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Why would each subscale carry a different weighting?
:::

The formula for calculating attitude is:

$$Attitude = 0.25 \times informed + 0.35 \times campaign + 0.4 \times activism$$

Using this formula are you able to figure out how to create the attitude variables ? 

::: {style="border-left: 4px solid #00bfa5; background-color: #e0f2f1; padding: 10px; margin: 10px 0;"}
<strong>Hint:</strong> Use the mutate function !
:::
  

```{r Activity 4}
social_media_attitutde <- social_media %>% 
  mutate(attitude = 0.25*polit_informed +
                    0.35*polit_campaign +
                    0.4*polit_activism)

```

Lets save this data so that we can use it next week: 

```{r}
write.csv(social_media_attitutde, file = here("Output", "PSYC2001-social-media-attitutde.csv"))
```


## Activity 3 - looking for straight lines

Now lets visualise this in R. 

We’re going to explore our data to see if it is suitable for a pearson correlation analysis. Remember, we use a pearson correlation analysis when we think a straight line is a good enough description of the relationship between our variables.

```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="MFW people don't visualise their data"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Correlations_in_R/No%20visualisation%20is%20bad.jpg?raw=true")
```


Let's use `ggplot` to  make scatter plots to show the relationships between age, time_on_social and attitude. You should wind up with 3 scatterplots.

```{r Scatterplot of age and time_on_social, warning=FALSE}
social_media_attitutde %>% 
  ggplot(aes(y = age, x = time_on_social)) +
  geom_point(colour = "orange") +
  labs(x = "Age", y = "Time on social") +
  theme_classic() 
```

```{r Scatterplot of age and attitude, warning=FALSE}
social_media_attitutde %>% 
  ggplot(aes(y = age, x = attitude)) +
  geom_point(colour = "blue") +
  labs(x = "Age", y = "Attitude") +
  theme_classic() 
```

```{r Scatterplot of time_on_social and attitude, warning=FALSE}
social_media_attitutde %>% 
  ggplot(aes(y = time_on_social, x = attitude)) +
  geom_point(colour = "red") +
  labs(x = "Time on social", y = "Attitude") +
  theme_classic() 
```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Do the relationships look linear or non-linear? Which correlation analyses would you be more cautious about running?
:::
  
## Conducting correlations

Let’s find out if there is a statistically significant correlation between `time_on_social` and `attitude`. To do this we can use the `cor.test()` function. This function takes in a formula 
of where the right hand side specifies the two numeric variables to be correlated.

```{r Correlations between time_on_social and attitude}
cor.test(formula = ~time_on_social + attitude, data = social_media_attitutde, use = "complete.obs") #formula contains both numeric variables on the right hand side.
#use = "complete.obs" removes all NA values from the correlation. 
```


::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Have a look at the output in the console. What is the relationship between `time_on_social` and `attitude`? Can you reject the null hypothesis? Was your hypothesis supported? What does the ‘df’ value tell you about the sample size? Why is it that number?
:::


## Writing up results and conclusions

Now lets have a go at writing up the results of the correlation that we have conducted: 

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Results:</strong> A pearson correlation was performed to evaluate the relationship between political attitude and time spent on social media. It was found that there was a strong positive correlation between political attitude and time spent on social media (r(56) = 0.51, p < 0.01).
:::

Please have a go at writing the conclusion:

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Conclusions:</strong> Your conclusion will go here !
:::

## Activity 4 - Exploring the data

Now we can harness the power of R to perform multiple correlations all at the same time ahhhhhhhhhh !

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> First what other relationships do you think could exist between variables in this dataset ? Make two hypotheses and discuss them with your tutor.
:::

Create a scatterplot of the variables you have hypothesised using the code blocks below:

```{r Scatterplot 1, eval = FALSE}
social_media_attitutde %>% 
  ggplot() +
  geom_point() +
  labs() +...
```

```{r Scatterplot 2, eval = FALSE}
social_media_attitutde %>% 
  ggplot() +
  geom_point() +
  labs() +...
```

Now compute correlations between these your chosen variables. 

```{r Correlation 1, eval = FALSE}
cor.test(x = , y =  use = "complete.obs")

```

```{r Correlation 2, eval = FALSE}
cor.test(x = , y =  use = "complete.obs")

```

## Writing up results and conclusions

Have a go at writing up the results and conclusions for the correlations you have just conducted. Make sure to use the previous results as a guide.

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Results:</strong> Your results will go here !
:::

Now have a try at the conclusion. 

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Conclusions:</strong> Your conclusion will go here !
:::

## You are Free!

Well done guys you have survived another computing tutorial! Next lesson we will be conducting linear regression on this data. 


```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="It almost over (I promise)"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Correlations_in_R/Statistical%20trauma.gif?raw=true")
```





<!--chapter:end:Correlations_in_R.Rmd-->

# Linear regression in R

This week, we are going to continue to explore relationships between continuous variables, using simple linear regression.

## Checking installation and loading packages

As usual we first always check and load in our required packages.

```{r message = FALSE, warning= FALSE}
# Check if packages are installed, if not install.
if(!require(here)) install.packages('here') #checks if a package is installed and installs it if required.
if(!require(tidyverse)) install.packages('tidyverse')
if(!require(ggplot2)) install.packages('ggplot2')

library(here) #loads in the specified package
library(tidyverse)
library(ggplot2)
```


## Investigating linear regressions

Last week we looked at the correlation between  social media use, age and political activism. Today we will be using a simple linear regression to determine if political attitude can predict the amount of time spent on social media.

Here is a recap of the variables:

- age – age in years
- time_on_social – average hours/day on social media (self-report diary)

Political attitude which is composed of the subscales: 

- informed – how politically informed they feel (e.g., read news daily)
- campaign – how much they engage in campaign-related discussion
- activism – involvement in activism (e.g., protests, petitions)


## Activity 1 - Formulate your research question

Before we begin we first need to load in our dataset for today. This will be there `PSYC2001-social-media-attitutde` dataset which includes the `attitude` variable we created last week.  Are you able to load this dataset  ? 

```{r }
social_media <- read.csv(file = here("Data","PSYC2001-social-media-attitutde.csv")) #reads in CSV files
```

Will social media use be predicted by political activism ? Which direction do you think this effect will be in ? 

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What is a reasonable null and alternate hypothesis? The class will come up with a hypothesis.
:::

## Linear regression in R 

The difference between a regression and correlation is that instead of looking for a statistical relationship between x and y, in regression we are asking if we can predict what y will be, under circumstances we are stuck on a desert island, and we only have access to x.

Let's run a simple linear regression to determine how well political attitude predicts the amount of time spent on social media. To do this we can we can fit a linear regression model using the `lm()` function in R. Please make sure to have a look at this function using the `?` syntax before moving on. To fit our linear regression model we have to specify a formula, similar to how we did in this [tutorial](#independent-samples-t-test). Here the left side in the formula is the dependent variable (`time_on_social`) whereas the right side is the independent/predictor variable (`attitude`). 

```{r}
model<-lm(formula = time_on_social ~ attitude, data = social_media_attitutde ) #since time_on_social is on the left side it is being predicted by attitude 
```

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Use the R output to complete the following regression equation:
attitude’ = a + beta*time_on_social.
:::

But think for a second... where is our R^2^ value and our p value ? 

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Can we make interpretations about the linear regression without these ?
:::

We can get the **R^2^ and p value** by generating a summary of our linear regression model. To do this we simply use the `summary()` function. 

```{r}
summary(model)
```

::: {style="border-left: 4px solid #2196F3; background-color: #E7F3FE; padding: 10px; margin: 10px 0;"}
<strong>Info:</strong> There are additional statistics that are output from the `lm()` model. They are not critical for interpretation at this point. If you want to know more please see [here](https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R)
:::

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> Can we make interpretations about the linear regression without these ?
:::

::: {style="border-left: 4px solid #9C27B0; background-color: #F3E5F5; padding: 10px; margin: 10px 0;"}
<strong>Question:</strong> What does this output mean ? What does the R^2^ mean ? How does the R compare to the results of your correlation analysis? Why is this?
:::

## Writing up results and conclusions

Finally, we are going to write up the results and conclusion of the linear regression we have just conducted. 

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Results:</strong> A simple linear regression was performed to evaluate the extent that political attitude predicted the amount of time spent on social media. It was found that political attitude weakly predicted the amount of time spent on social media (F(1,56) = 19.95,p < 0.01, R^2^ = 0.26).
:::

What can we conclude from these results? Please write this up!

::: {style="border-left: 4px solid #8B5E3C; background-color: #F5F5DC; padding: 10px; margin: 10px 0;"}
<strong>Conclusions:</strong> Your conclusions go here !
:::

## Plotting our results

To show how well a simple linear regression captures the relationship between two variables you should simply include a straight line of 'best fit' on a scatterplot. This is incredibly useful at visually informing the reader of the linear predicitve relationship between variables. Surprisingly, most people (including researchers) are quite bad at guessing the strength of linear relationships using scatterplots alone !

## Activity 2 - Plotting straight lines

The linear regression line can be added to a `ggplot` graph by using the `geom_smooth` function. Please help to do this by filling in the code below.  

```{r warning= FALSE, message = FALSE}
social_media_attitutde %>% 
  ggplot(aes(x = time_on_social, y = attitude)) +
  geom_point(colour = "red") +
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "Time on social", y = "Attitude", colour = "black") +
  theme_classic()
```


## You are Free!
  
Well done guys you have survived another computing tutorial. Please check with your tutor that you understand everything !


```{r, echo=FALSE, out.width="50%", fig.align='center', fig.cap="It almost over (I promise)"}
knitr::include_graphics("https://github.com/BsoCool/UNSW-PSYC2001/blob/main/All%20images%20for%20computing%20tutorials/Linear_regression_in_R/Done.jpg?raw=true")
```





<!--chapter:end:Linear_regression_in_R.Rmd-->

